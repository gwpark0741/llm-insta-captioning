"""
generate_trainval.py

- Qwen2.5-3B-Instructë¡œ COCO(train+val í˜¼í•©) ì›ë³¸ 5ìº¡ì…˜(cap1~cap5)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„
  Instagram ìŠ¤íƒ€ì¼ ì˜ì–´ caption + hashtags(JSON)ë¥¼ ìƒì„±í•˜ê³ ,
  ê²°ê³¼/ì‹¤íŒ¨ ë¡œê·¸ë¥¼ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤.

ì‹¤í–‰:
  python3 scripts/generate_trainval.py
"""

# ======================
# 1) í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª¨ë‘ ë¡œë“œ
# ======================
import re
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple

import pandas as pd
from tqdm import tqdm

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


# ======================
# 2) ê²½ë¡œ/íŒŒì¼ëª…/ì„¤ì •ê°’ ì •ì˜
# ======================
PROJECT_ROOT = Path.cwd()
DATA_DIR = PROJECT_ROOT / "data"
OUTPUT_DIR = PROJECT_ROOT / "outputs"

COCO_CSV = DATA_DIR / "coco_bottle_bowl_5caps.csv"
FEWSHOT_JSON = DATA_DIR / "insta_caption_5_en_kr.json"

OUT_PATH = OUTPUT_DIR / "insta_en_generated_trainval.csv"
FAIL_PATH = OUTPUT_DIR / "insta_en_failed_trainval.csv"

MODEL_NAME = "Qwen/Qwen2.5-3B-Instruct"

BATCH_SIZE = 4
SAVE_EVERY = 100
MAX_NEW_TOKENS = 120

TEMP_MAIN = 0.8
TOP_P_MAIN = 0.9
TEMP_RETRY = 0.55
TOP_P_RETRY = 0.85

DTYPE = torch.float16

TARGET_SPLITS = {"train2017", "val2017"}


# ======================
# ìœ í‹¸: ë¡œê·¸
# ======================

# [log] ë””ë²„ê¹…ì„ ì‰½ê²Œ í•˜ê¸° ìœ„í•´ ì¶œë ¥ ë©”ì‹œì§€ë¥¼ ì¦‰ì‹œ flush í•˜ëŠ” ë¡œê·¸ í•¨ìˆ˜
def log(msg: str) -> None:
    print(msg, flush=True)


# ======================
# ìœ í‹¸: ê²½ë¡œ í™•ì¸/í´ë” ìƒì„±
# ======================

# [ensure_paths] ì…ë ¥ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³  outputs í´ë”ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def ensure_paths() -> None:
    """í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸ + outputs í´ë” ìƒì„±."""
    if not COCO_CSV.exists():
        raise FileNotFoundError(f"[Missing] COCO_CSV not found: {COCO_CSV}")
    if not FEWSHOT_JSON.exists():
        raise FileNotFoundError(f"[Missing] FEWSHOT_JSON not found: {FEWSHOT_JSON}")
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


# ======================
# ë°ì´í„° ë¡œë“œ
# ======================

# [load_coco_dataframe] COCO CSVë¥¼ ë¡œë“œí•˜ê³ , íŒŒì´í”„ë¼ì¸ì— í•„ìš”í•œ ìµœì†Œ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜
def load_coco_dataframe() -> pd.DataFrame:
    """COCO CSV ë¡œë“œ ë° í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦."""
    df = pd.read_csv(COCO_CSV)

    required_cols = ["image_id", "split", "cap1", "cap2", "cap3", "cap4", "cap5"]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        raise ValueError(
            f"[COCO CSV] missing columns: {missing}\n"
            f"-> í˜„ì¬ ì»¬ëŸ¼: {df.columns.tolist()}"
        )
    return df


# ======================
# í•´ì‹œíƒœê·¸ ì •ê·œí™”
# ======================

# [normalize_hashtags] í•´ì‹œíƒœê·¸ ë‚´ì˜ ê³µë°±ë§Œ ì œê±°í•˜ì—¬ '#game night' -> '#gamenight' í˜•íƒœë¡œ ë§Œë“œëŠ” í•¨ìˆ˜
def normalize_hashtags(tags: str) -> str:
    """
    í•´ì‹œíƒœê·¸ ì •ê·œí™”: ê³µë°± ì œê±°ë§Œ
    ì˜ˆ) "#game night" -> "#gamenight"
    """
    if not isinstance(tags, str):
        return ""
    s = tags.strip()
    if not s:
        return ""

    parts = s.split("#")
    out = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        out.append("#" + p.replace(" ", ""))
    return " ".join(out)


# [strip_hashtags_from_caption] ìº¡ì…˜ì— ì‹¤ìˆ˜ë¡œ ì„ì¸ '#...' í† í°ì„ ì œê±°í•´ ìº¡ì…˜ í…ìŠ¤íŠ¸ë¥¼ ê¹¨ë—í•˜ê²Œ ë§Œë“œëŠ” í•¨ìˆ˜
def strip_hashtags_from_caption(caption: str) -> str:
    """caption ì•ˆì— ì‹¤ìˆ˜ë¡œ ë“¤ì–´ê°„ í•´ì‹œíƒœê·¸ ì œê±°."""
    if not isinstance(caption, str):
        return ""
    return re.sub(r"#\S+", "", caption).strip()


# ======================
# few-shot ë¡œë“œ
# ======================

# [load_fewshot_examples] few-shot JSONì—ì„œ ì˜ì–´ ìº¡ì…˜/í•´ì‹œíƒœê·¸ë§Œ ë½‘ì•„ ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“œëŠ” í•¨ìˆ˜
def load_fewshot_examples() -> List[Dict[str, str]]:
    """few-shot JSONì—ì„œ en_caption/en_hashtagsë§Œ ì¶”ì¶œ."""
    with open(FEWSHOT_JSON, "r", encoding="utf-8") as f:
        data = json.load(f)

    if not isinstance(data, list):
        raise ValueError(f"[Fewshot JSON] must be a list. got={type(data)}")

    examples: List[Dict[str, str]] = []
    skipped = 0

    for ex in data:
        if not isinstance(ex, dict):
            skipped += 1
            continue

        en_cap = ex.get("en_caption")
        en_tags = ex.get("en_hashtags")

        if isinstance(en_cap, str) and en_cap.strip() and isinstance(en_tags, str) and en_tags.strip():
            examples.append({
                "caption": en_cap.strip(),
                "hashtags": normalize_hashtags(en_tags),
            })
        else:
            skipped += 1

    if not examples:
        raise ValueError("[Fewshot JSON] no valid examples. check keys: en_caption, en_hashtags")

    log(f"âœ… Few-shot loaded: {len(examples)} (skipped={skipped})")
    return examples


# ======================
# í”„ë¡¬í”„íŠ¸ ë¹Œë”
# ======================

# [build_scene_desc] cap1~cap5ë¥¼ "ì •í™•íˆ 5ì¤„"ë¡œ ë§Œë“¤ì–´ user promptì— ë„£ê¸° ìœ„í•œ ì…ë ¥ ë¬¸ìì—´ì„ ë§Œë“œëŠ” í•¨ìˆ˜
def build_scene_desc(row: pd.Series) -> str:
    """cap1~cap5ë¥¼ 5ì¤„ë¡œ í•©ì³ user prompt ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©."""
    caps = []
    for k in ["cap1", "cap2", "cap3", "cap4", "cap5"]:
        v = row.get(k, "")
        if isinstance(v, str) and v.strip() and v.lower() != "nan":
            caps.append(v.strip())
        else:
            caps.append("")
    return "\n".join(caps)


# [build_system_prompt] ëª¨ë¸ ìŠ¤íƒ€ì¼ì„ ê³ ì •í•˜ê¸° ìœ„í•œ system promptë¥¼ êµ¬ì„±(ê¸ˆì§€ì‚¬í•­/ì¶œë ¥í˜•ì‹/ì˜ˆì‹œ í¬í•¨)í•˜ëŠ” í•¨ìˆ˜
def build_system_prompt(fewshots: List[Dict[str, str]]) -> str:
    """few-shotì„ systemì— ë„£ì–´ ìŠ¤íƒ€ì¼ ê³ ì •."""
    shots_txt = ""
    for i, ex in enumerate(fewshots, 1):
        shots_txt += (
            f"Example {i}:\n"
            f'{{"caption": "{ex["caption"]}", "hashtags": "{ex["hashtags"]}"}}\n'
        )

    return f"""You are NOT describing an image.
You are the person who posted this photo on Instagram.

Persona:
You are casually sharing a moment from your own daily life.
You are not explaining what is visible.
You are not analyzing a scene.
You are writing like a real Instagram user who lived this moment.

Writing mindset (very important):
- Write in first-person perspective implicitly (without saying "I" too much).
- Capture how the moment felt, not what was in the photo.
- Think: â€œWhy did I feel like posting this?â€
- The caption should feel personal, natural, and unforced.

Core style:
- Warm, cozy, lifestyle-focused.
- Emotions, atmosphere, quiet moments, shared time.
- Everyday feelings that people relate to.
- Use sensory language (light, warmth, calm, comfort, rhythm of the day).

Strong prohibitions (must NOT do):
- Do NOT describe the image like a report or dataset caption.
- Do NOT list objects, people, or actions factually.
- Do NOT summarize the scene.
- Do NOT mention the task, the input captions, or the generation process.
- Do NOT say things like â€œthis image showsâ€, â€œbased on the descriptionsâ€, or similar.
- Do NOT include hashtags inside the caption text.

Output rules:
- Language: English only.
- Caption: 2â€“4 sentences.
- Emotional, personal, and Instagram-native.
- Hashtags: 5â€“7 hashtags, ONE separate line, all lowercase.
- Focus hashtags on lifestyle, mood, daily moments (not object names).

Output format (strict):
- Output exactly ONE JSON object.
- No explanations, no commentary, no extra text.
- Format:
  {{"caption":"...","hashtags":"#... #... #..."}}

Style references (few-shot examples):
{shots_txt}
"""


# [build_user_prompt] ì›ë³¸ 5ì¤„ ìº¡ì…˜(scene_desc)ì„ ë„£ì–´ ëª¨ë¸ì—ê²Œ "JSONë§Œ ì¶œë ¥"í•˜ë„ë¡ ìš”ì²­í•˜ëŠ” user promptë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜
def build_user_prompt(scene_desc: str) -> str:
    """ì›ë³¸ 5ìº¡ì…˜ ì œê³µ."""
    return f"""Here are 5 captions describing the same scene (exactly 5 lines):
{scene_desc}

Now produce the JSON output.
"""


# ======================
# JSON íŒŒì‹±(ë” ê²¬ê³ í•˜ê²Œ)
# ======================

# [parse_json_from_text] ëª¨ë¸ ì¶œë ¥ì—ì„œ JSONë§Œ ìµœëŒ€í•œ ì•ˆì •ì ìœ¼ë¡œ ì¶”ì¶œí•´ dictë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def parse_json_from_text(text: str) -> Optional[Dict[str, str]]:
    """
    1) ì²« '{'ì™€ ë§ˆì§€ë§‰ '}'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í›„ë³´ JSONì„ ì¡ëŠ”ë‹¤.
    2) json.loads ì‹œë„.
    3) ì‹¤íŒ¨í•˜ë©´ ë§ˆì§€ë§‰ '}'ë¥¼ ì•ë‹¹ê¸°ë©° ëª‡ ë²ˆ ë” ì‹œë„.
    """
    if not isinstance(text, str):
        return None

    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1 or end <= start:
        return None

    candidate = text[start:end + 1].strip()

    # ì—¬ëŸ¬ '}'ê°€ ìˆì„ ìˆ˜ ìˆì–´ ì ì§„ì ìœ¼ë¡œ ì¤„ì´ë©° ì‹œë„
    for _ in range(5):
        try:
            obj = json.loads(candidate)
            cap = str(obj.get("caption", "")).strip()
            tags = str(obj.get("hashtags", "")).strip()
            return {"caption": cap, "hashtags": tags}
        except Exception:
            end2 = candidate.rfind("}", 0, len(candidate) - 1)
            if end2 == -1:
                break
            candidate = candidate[:end2 + 1].strip()

    return None


# [validate_output] íŒŒì‹±ëœ ê²°ê³¼ê°€ ìµœì†Œ í’ˆì§ˆ ê¸°ì¤€(ë¬¸ì¥/í•´ì‹œíƒœê·¸ ê°œìˆ˜ ë“±)ì„ ë§Œì¡±í•˜ëŠ”ì§€ ê²€ì‚¬í•˜ëŠ” í•¨ìˆ˜
def validate_output(obj: Optional[Dict[str, str]]) -> bool:
    """ê¸°ë³¸ í’ˆì§ˆ ê²€ì¦."""
    if obj is None:
        return False
    cap = obj.get("caption", "")
    tags = obj.get("hashtags", "")
    if not cap or not tags:
        return False

    tag_list = [t for t in tags.split() if t.startswith("#")]
    if len(tag_list) < 3:
        return False

    sent_cnt = len(re.findall(r"[.!?]", cap))
    if sent_cnt < 1:
        return False

    return True


# ======================
# ëª¨ë¸ ë¡œë“œ
# ======================

# [load_model_and_tokenizer] í† í¬ë‚˜ì´ì €/ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , padding ì„¤ì •ê³¼ pad_token_idë¥¼ ì•ˆì „í•˜ê²Œ ë§ì¶”ëŠ” í•¨ìˆ˜
def load_model_and_tokenizer() -> Tuple[AutoTokenizer, AutoModelForCausalLM]:
    """Qwen2.5 ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ + padding ì„¤ì •."""
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

    # decoder-only ëª¨ë¸ì—ì„œëŠ” left paddingì´ ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì „í•¨
    tokenizer.padding_side = "left"

    # pad_tokenì´ ì—†ìœ¼ë©´ eosë¡œ ëŒ€ì²´(íŒ¨ë”© ê´€ë ¨ ì˜¤ë¥˜/ê²½ê³  ë°©ì§€)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        device_map="auto",
        dtype=DTYPE,
    )
    model.eval()

    # ëª¨ë¸ì˜ pad_token_idê°€ ë¹„ì–´ ìˆìœ¼ë©´ í† í¬ë‚˜ì´ì € ê°’ìœ¼ë¡œ ì§€ì •
    if model.generation_config.pad_token_id is None:
        model.generation_config.pad_token_id = tokenizer.pad_token_id

    log(f"âœ… Loaded model: {MODEL_NAME}")
    return tokenizer, model


# ======================
# ë°°ì¹˜ ìƒì„±(ìƒ˜í”Œë³„ slice ì•ˆì „ ë²„ì „)
# ======================

# [generate_batch] ì—¬ëŸ¬ ìƒ˜í”Œì„ ë°°ì¹˜ë¡œ ìƒì„±í•˜ê³ , ê° ìƒ˜í”Œì˜ ì‹¤ì œ ì…ë ¥ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ìƒì„± í…ìŠ¤íŠ¸ë§Œ decodeí•˜ëŠ” í•¨ìˆ˜
@torch.inference_mode()
def generate_batch(
    tokenizer: AutoTokenizer,
    model: AutoModelForCausalLM,
    scene_desc_list: List[str],
    system_prompt: str,
    max_new_tokens: int,
    temperature: float,
    top_p: float,
) -> List[str]:
    """ë°°ì¹˜ ì¶”ë¡  í›„, ìƒ˜í”Œë³„ ì…ë ¥ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ decode."""
    prompts = []
    for scene_desc in scene_desc_list:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": build_user_prompt(scene_desc)},
        ]
        prompts.append(
            tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        )

    inputs = tokenizer(
        prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
    ).to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
        top_p=top_p,
        repetition_penalty=1.05,
        eos_token_id=tokenizer.eos_token_id,
    )

    # ìƒ˜í”Œë³„ ì‹¤ì œ ì…ë ¥ ê¸¸ì´(=attention_mask í•©)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ slicing
    attn = inputs["attention_mask"]
    input_lens = attn.sum(dim=1).tolist()

    results = []
    for i in range(outputs.size(0)):
        gen_ids = outputs[i][int(input_lens[i]):]
        results.append(tokenizer.decode(gen_ids, skip_special_tokens=True))

    return results


# ======================
# ì €ì¥ ìœ í‹¸
# ======================

# [append_fail_rows] ì‹¤íŒ¨ ìƒ˜í”Œ ë¡œê·¸ë¥¼ CSVì— ëˆ„ì  ì €ì¥í•˜ëŠ” í•¨ìˆ˜(ì¤‘ë‹¨/ì¬ê°œ ì‹œì—ë„ ê¸°ë¡ ìœ ì§€)
def append_fail_rows(fail_rows: List[Dict], path: Path) -> None:
    """ì‹¤íŒ¨ ë¡œê·¸ë¥¼ CSVì— append ì €ì¥."""
    if not fail_rows:
        return
    df_new = pd.DataFrame(fail_rows)
    if path.exists():
        df_prev = pd.read_csv(path)
        pd.concat([df_prev, df_new], ignore_index=True).to_csv(path, index=False, encoding="utf-8-sig")
    else:
        df_new.to_csv(path, index=False, encoding="utf-8-sig")


# [save_checkpoint] ê²°ê³¼ CSVë¥¼ ì €ì¥í•˜ê³ , ì‹¤íŒ¨ ë¡œê·¸ë„ í•¨ê»˜ ë°˜ì˜í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í•¨ìˆ˜
def save_checkpoint(rows_out: List[Dict], fail_buffer: List[Dict]) -> None:
    """ê²°ê³¼/ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥."""
    pd.DataFrame(rows_out).to_csv(OUT_PATH, index=False, encoding="utf-8-sig")
    append_fail_rows(fail_buffer, FAIL_PATH)


# [make_unique_key] resume(ì´ì–´í•˜ê¸°) ì¤‘ ì¤‘ë³µ ìƒì„±ì„ ë§‰ê¸° ìœ„í•´ split+image_idë¥¼ ìœ ë‹ˆí¬ í‚¤ë¡œ ë§Œë“œëŠ” í•¨ìˆ˜
def make_unique_key(row: pd.Series) -> str:
    """resume ì•ˆì „ì„± ìœ„í•´ split+image_idë¥¼ í‚¤ë¡œ ì‚¬ìš©."""
    return f"{row.get('split','')}_{int(row.get('image_id'))}"


# ======================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ======================

# [run_generation] ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰(ë°ì´í„° ë¡œë“œ â†’ í”„ë¡¬í”„íŠ¸/ëª¨ë¸ ì¤€ë¹„ â†’ ìƒì„± ë£¨í”„ â†’ ì €ì¥/ì¬ê°œ)
def run_generation() -> None:
    ensure_paths()

    coco_df = load_coco_dataframe()
    fewshot_examples = load_fewshot_examples()
    system_prompt = build_system_prompt(fewshot_examples[:5])

    tokenizer, model = load_model_and_tokenizer()

    # train+val ëª¨ë‘ ëŒ€ìƒìœ¼ë¡œ ìƒì„±
    gen_df = coco_df[coco_df["split"].isin(TARGET_SPLITS)].copy().reset_index(drop=True)
    log(f"âœ… Target splits: {sorted(TARGET_SPLITS)}")
    log(f"âœ… Total rows to generate: {len(gen_df)}")

    # resume ì¤€ë¹„
    rows_out: List[Dict] = []
    done_keys = set()

    if OUT_PATH.exists():
        prev = pd.read_csv(OUT_PATH)
        rows_out = prev.to_dict("records")
        if "split" in prev.columns and "image_id" in prev.columns:
            done_keys = set(
                (prev["split"].astype(str) + "_" + prev["image_id"].astype(int).astype(str)).tolist()
            )
        log(f"ğŸ” Resume mode: already generated {len(done_keys)} rows.")

    cnt_total = cnt_success = cnt_retry = cnt_fail = 0
    last_saved = len(rows_out)
    fail_buffer: List[Dict] = []

    pbar = tqdm(range(0, len(gen_df), BATCH_SIZE), desc="Generating train+val")

    for start in pbar:
        batch = gen_df.iloc[start:start + BATCH_SIZE]

        # ì´ë¯¸ ìƒì„±ëœ í‚¤ëŠ” ì œì™¸(ì¤‘ë³µ ë°©ì§€)
        batch = batch[~batch.apply(make_unique_key, axis=1).isin(done_keys)]
        if len(batch) == 0:
            continue

        scene_desc_list = [build_scene_desc(r) for _, r in batch.iterrows()]

        # ë°°ì¹˜ ìƒì„± (OOM ë“± ëŸ°íƒ€ì„ ì—ëŸ¬ ê°€ëŠ¥)
        try:
            decoded_list = generate_batch(
                tokenizer, model, scene_desc_list, system_prompt,
                max_new_tokens=MAX_NEW_TOKENS,
                temperature=TEMP_MAIN,
                top_p=TOP_P_MAIN,
            )
        except RuntimeError as e:
            log("\nâŒ RuntimeError during generation (likely OOM).")
            log("   -> Reduce BATCH_SIZE or MAX_NEW_TOKENS.")
            log(f"   Error: {e}")
            raise

        # ìƒ˜í”Œë³„ íŒŒì‹±/ê²€ì¦/ì¬ì‹œë„
        for (_, r), decoded in zip(batch.iterrows(), decoded_list):
            cnt_total += 1
            attempts = 1

            obj = parse_json_from_text(decoded)

            # 1íšŒ ì¬ì‹œë„(temperature ë‚®ì¶° ì•ˆì •í™”)
            if not validate_output(obj):
                cnt_retry += 1
                attempts = 2
                decoded_retry = generate_batch(
                    tokenizer, model, [build_scene_desc(r)], system_prompt,
                    max_new_tokens=MAX_NEW_TOKENS,
                    temperature=TEMP_RETRY,
                    top_p=TOP_P_RETRY,
                )[0]
                obj_retry = parse_json_from_text(decoded_retry)
                if validate_output(obj_retry):
                    obj = obj_retry
                    decoded = decoded_retry
                else:
                    cnt_fail += 1

            ok = validate_output(obj)

            # ì›ë³¸ rowì˜ ëª¨ë“  ì»¬ëŸ¼ ë³´ì¡´ + ìƒì„± ë©”íƒ€ì •ë³´ ì¶”ê°€
            out_row = r.to_dict()
            out_row.update({
                "gen_model": MODEL_NAME,
                "gen_ok": bool(ok),
                "gen_attempts": int(attempts),
                "gen_max_new_tokens": int(MAX_NEW_TOKENS),
                "gen_temperature": float(TEMP_MAIN if attempts == 1 else TEMP_RETRY),
                "gen_top_p": float(TOP_P_MAIN if attempts == 1 else TOP_P_RETRY),
                "raw_output": (decoded or "")[:5000],  # ë””ë²„ê¹…ìš© raw ì €ì¥
            })

            if obj:
                cap = strip_hashtags_from_caption(obj.get("caption", ""))
                tags = normalize_hashtags(obj.get("hashtags", ""))
            else:
                cap, tags = "", ""

            out_row["en_caption"] = cap if ok else ""
            out_row["en_hashtags"] = tags if ok else ""

            rows_out.append(out_row)
            done_keys.add(make_unique_key(r))

            if ok:
                cnt_success += 1
            else:
                fail_buffer.append({
                    **r.to_dict(),
                    "gen_model": MODEL_NAME,
                    "raw_output": (decoded or "")[:5000],
                })

            pbar.set_postfix({
                "done": cnt_total,
                "success": cnt_success,
                "retry": cnt_retry,
                "fail": cnt_fail,
                "saved_total": len(rows_out),
            })

        # ì£¼ê¸° ì €ì¥(ì¤‘ë‹¨ ëŒ€ë¹„)
        if (len(rows_out) - last_saved) >= SAVE_EVERY:
            save_checkpoint(rows_out, fail_buffer)
            last_saved = len(rows_out)
            fail_buffer = []
            log(f"\n[Checkpoint] saved_total={len(rows_out)} | done={cnt_total} | "
                f"success={cnt_success} | retry={cnt_retry} | fail={cnt_fail}")

    # ìµœì¢… ì €ì¥
    save_checkpoint(rows_out, fail_buffer)

    log("\nâœ… Generation finished")
    log(f"Processed={cnt_total}, Success={cnt_success}, Retry={cnt_retry}, Fail={cnt_fail}")
    log(f"Saved to: {OUT_PATH}")
    log(f"Failed log saved to: {FAIL_PATH}")


# [main] ì „ì²´ ì‹¤í–‰ì„ try/exceptë¡œ ê°ì‹¸ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•˜ëŠ” ì—”íŠ¸ë¦¬ í•¨ìˆ˜
def main() -> None:
    try:
        run_generation()
    except Exception as e:
        log("\n====================")
        log("âŒ Pipeline crashed")
        log("====================")
        log(f"Error type: {type(e).__name__}")
        log(f"Message: {e}")
        log("Tip:")
        log(" - data/ í´ë”ì— coco_bottle_bowl_5caps.csv, insta_caption_5_en_kr.json ì¡´ì¬?")
        log(" - venv í™œì„±í™” ìƒíƒœ? (.venv)")
        log(" - OOMì´ë©´ BATCH_SIZE=2, MAX_NEW_TOKENS=80ë¡œ ë‚®ì¶”ê¸°")
        raise


if __name__ == "__main__":
    main()
